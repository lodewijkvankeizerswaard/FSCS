#!/bin/bash

#SBATCH --partition=gpu_titanrtx_shared_course
#SBATCH --gres=gpu:1
#SBATCH --job-name=CelebTitan
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --time=12:00:00
#SBATCH --mem=32000M
#SBATCH --array=1-2%1
#SBATCH --output=slurm_array_CELEBA_%A_%a.out

module purge
module load 2021
module load Anaconda3/2021.05

DATASET="celeba"

cd $HOME/FSCS
# Copy dataset
cp -r data/$DATASET $TMPDIR/$DATASET

# Activate your environment
source activate FSCS

HPARAMS_FILE=$HOME/FSCS/lisa_jobs/celeba/celeba_hyperparams.txt

# Run your code
srun python -u train_model.py --dataset $DATASET --batch_size 128 --epochs 10 --dataset_root $TMPDIR \
               $(head -$SLURM_ARRAY_TASK_ID $HPARAMS_FILE | tail -1)